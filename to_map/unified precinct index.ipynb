{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jaro\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    def ld_nh(path):\n",
    "        for f in os.listdir(path):\n",
    "            if not f.startswith('.'):\n",
    "                yield f\n",
    "    return list (ld_nh(path))\n",
    "\n",
    "def show_county_data(county, data):\n",
    "    d = data[[c for c in data.columns if 'geo' not in c]].copy()\n",
    "    try: out = d[d.county==county].sort_values(d.columns[1])\n",
    "    except: out = d[d.county==county].sort_values(d.columns[1])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `shapes` = precincts geojson\n",
    "- `part` = pre-2020 demographic participation \n",
    "- `res` = nov 2020 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the native indexes for mapping, so don't reset or drop them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 6 rows found with unique loc_prec values. Overwriting prec_shp...\n",
      "> 2 with unique geometry rows share names! Adding indexes to names...\n",
      "  > PATRIOTS PARK (1/2)\n",
      "  > PATRIOTS PARK (2/2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>precinct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>appling</td>\n",
       "      <td>1b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>appling</td>\n",
       "      <td>1c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>appling</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       county precinct\n",
       "1450  appling       1b\n",
       "1451  appling       1c\n",
       "1452  appling        2"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHAPES DATA\n",
    "\n",
    "shapes = gp.read_file('electoral_precincts/2018Precincts.shp')\n",
    "shapes = shapes[list(shapes.columns[:4])+['geometry']]\n",
    "# rows where loc_prec contains a value not the same as prec_shp\n",
    "mismatch = np.where(((shapes['locality']+','+shapes['prec_shp']\n",
    "                  ).str.lower()!=shapes['loc_prec'].str.lower()))\n",
    "print('>', len(mismatch[0]), 'rows found with unique loc_prec values. Overwriting prec_shp...')\n",
    "# replace odd prec_shp values (they are duplicated in prec_elect)\n",
    "shapes.prec_elec = shapes.loc_prec.apply(lambda x: x.split(',')[1])\n",
    "shapes.drop('loc_prec', axis=1, inplace=True)\n",
    "\n",
    "# dna = Duplicate Named Area (areas with unique geometry that )\n",
    "dna = shapes[shapes.duplicated(['locality', 'prec_shp'])].copy()\n",
    "dup_idx = dna[dna.duplicated(['geometry'])].index\n",
    "dna.drop(dup_idx, inplace=True) # drop straight-up duplicates\n",
    "shapes.drop(dup_idx, inplace=True) # drop straight-up duplicates\n",
    "print('>', len(dna), 'with unique geometry rows share names! Adding indexes to names...')\n",
    "idxr = 1\n",
    "for d_idx in dna.index: # remainig rows have unique gemoetry\n",
    "    o_val = dna.loc[d_idx, 'prec_shp']\n",
    "    n_val = f'{o_val} ({idxr}/{len(dna)})'\n",
    "    shapes.loc[d_idx, 'prec_shp'] = n_val # assign back into shapes directly.\n",
    "    idxr += 1\n",
    "    print('  >', n_val)\n",
    "    \n",
    "# clean vals (force lowercase) and rename\n",
    "for c in shapes.columns[:3]:\n",
    "    shapes[c] = shapes[c].str.lower()\n",
    "shapes.rename(columns={'locality':'county'}, inplace=True)\n",
    "\n",
    "shapes[shapes.county=='appling'].sort_values('prec_shp').head(3)\n",
    "\n",
    "\n",
    "# OLD RUNOFF PARTICPATION DATA\n",
    "\n",
    "part = pd.read_csv('../recent_runoffs/2018_november_cleaned/all_precincts_participation.csv')\n",
    "part = part[list(part.columns[:3])]\n",
    "for c in part.columns: part[c] = part[c].str.lower()\n",
    "part.rename(columns={\n",
    "        'County':'county',\n",
    "        'PRECINCT ID': 'prec_id',\n",
    "        'PRECINCT DESCRIPTION': 'prec_desc'\n",
    "    }, inplace=True)\n",
    "\n",
    "part[part.county=='appling'].sort_values('prec_desc').head(3)\n",
    "\n",
    "\n",
    "# 2018 NOVEMBER RESULTS DATA\n",
    "\n",
    "res = pd.read_csv('../2020_november/all_precincts_joined/US Senate (Loeffler) - Special.csv')\n",
    "res = res[list(res.columns[:2])]\n",
    "res.rename(columns={k:k.lower() for k in res.columns}, inplace=True)\n",
    "for c in res.columns: res[c] = res[c].str.lower()\n",
    "    \n",
    "res[res.county=='appling'].sort_values('precinct').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual fixes - exact string replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE EXACT STRINGS in SHAPES DATA\n",
    "\n",
    "replace_strs = {\n",
    "    'hoggard mill': 'hoggards mill',\n",
    "    'south mill': 'south milledgeville',\n",
    "    'north mill': 'north milledgeville',\n",
    "    'bethlehem church - 211': 'bethlehem church',\n",
    "    'chattahoochee acvitity center': 'activity center' ,\n",
    "    'cjc': '#3 cjc',\n",
    "}\n",
    "for o, n in replace_strs.items():\n",
    "    for c in shapes.columns[:3]:\n",
    "        shapes[c] = shapes[c].str.replace(o, n)\n",
    "        \n",
    "# REPLACE EXACT STRINGS in PART DATA\n",
    "\n",
    "replace_strs = {\n",
    "    'austin \\(dun\\)': 'austin',\n",
    "    'avondale \\(avo\\)': 'avondale',\n",
    "    'lithonia \\(lit\\)': 'lithonia',\n",
    "    'woodward \\(bhavn\\)': 'woodward',\n",
    "    'fbc - flc': 'family life center',\n",
    "}\n",
    "for o, n in replace_strs.items():\n",
    "    for c in part.columns[:3]:\n",
    "        part[c] = part[c].str.replace(o, n)\n",
    "        \n",
    "# REPLACE EXACT STRINGS in RES DATA\n",
    "replace_strs = {\n",
    "    ' ':' '\n",
    "}\n",
    "for o, n in replace_strs.items():\n",
    "    for c in res.columns[:3]:\n",
    "        res[c] = res[c].str.replace(o, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## direct edits by iloc - warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RISKY â€” THESE MAY CHANGE!!\n",
    "part.loc[133, 'prec_desc'] = 'fairground'\n",
    "part.loc[358, 'prec_desc'] = 'eli whitney'\n",
    "part.loc[291, 'prec_desc'] = 'wilmington island presbyterian'\n",
    "part.loc[347, 'prec_desc'] = 'wilmington island united'\n",
    "\n",
    "res.loc[1136, 'precinct_id'] =  'bramlett elementary'\n",
    "res.loc[1137, 'precinct_id'] =  'westside middle'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specific county cleaning\n",
    "\n",
    "### strip numbers from barrow precincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD COLUMN FOR PARSED PRECINCT IDs EXACT STRINGS\n",
    "county='barrow'\n",
    "barrow_idx = show_county_data(county, res).index\n",
    "barrow_ids = res.loc[barrow_idx, 'precinct'].apply(lambda x: ' '.join(re.findall('[A-Za-z]*', x)))\n",
    "res.loc[barrow_idx, 'precinct'] = barrow_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### de-code rockdale precincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockdale_p_map = {'BA': 'Barkside',\n",
    " 'BT': 'Bethel',\n",
    " 'CO': 'Conyers',\n",
    " 'FI': 'Fieldstone',\n",
    " 'FS': 'Flat Shoals',\n",
    " 'HC': 'Honey Creek',\n",
    " 'HI': 'High Tower',\n",
    " 'LA': 'The Lakes',\n",
    " 'LO': 'Lorraine',\n",
    " 'MA': 'Magnet',\n",
    " 'MI': 'Milestead',\n",
    " 'OT': 'Olde Town',\n",
    " 'RO': 'Rockdale',\n",
    " 'SM': 'Smyrna',\n",
    " 'SP': 'St. Pius',\n",
    " 'ST': 'Stanton'}\n",
    "\n",
    "def convert_rockdale(ab):\n",
    "    ab = ab.upper()\n",
    "    if ab in rockdale_p_map.keys():\n",
    "        return rockdale_p_map[ab].lower()\n",
    "    else:\n",
    "        return ab.lower()\n",
    "    \n",
    "rock_idx = part[part.county=='rockdale'].index\n",
    "\n",
    "part.loc[rock_idx, 'prec_id'] = part.loc[rock_idx, 'prec_id'].apply(convert_rockdale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix spalding leading zero numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "spald = shapes[shapes.county=='spalding'].index\n",
    "\n",
    "def convert_spald(n):\n",
    "    if len(n)<2:\n",
    "        return'0'+n\n",
    "    else:\n",
    "        return n\n",
    "\n",
    "shapes.loc[spald, 'prec_shp'] = shapes.loc[spald, 'prec_shp'].apply(convert_spald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate shape index objects, looking for data from `part` and `res`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS LOOP searches for matches across datasets using multiple columns\n",
    "# (precinct id, precinct, prec_elec, prec_id, precinct description)\n",
    "# ideally, a perfect match is found. always search within a matching county.\n",
    "# if no perfect match for a given search, try fuzzy or custom searching... \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# to avoid importing numpy!\n",
    "\n",
    "        #   l = left data  |  r = right data\n",
    "\n",
    "def find_matches(l_df, r_df, min_score, p_groupby='county', testing=False):\n",
    "    match_hist = {} # to store the results and stats about matches\n",
    "    found = 0\n",
    "    customs = ['county', '_idx', '_mthd', '_fuzz'] # to exclude from search terms\n",
    "    ignore_vals = ['88888', '99999', 'nan']\n",
    "\n",
    "    l_df.fillna('nan', inplace=True)\n",
    "    r_df.fillna('nan', inplace=True)\n",
    "    \n",
    "    if not testing: iter_over = sorted(list(l_df[p_groupby].unique()))\n",
    "    else: iter_over=testing \n",
    "        \n",
    "    if 'prec_desc' in r_df.columns: # first round, so save init INDEX\n",
    "        l_df.reset_index(inplace=True)\n",
    "        l_df.rename(columns={'index':'SHAPE_idx'}, inplace=True)\n",
    "\n",
    "    for p_item in tqdm(iter_over):    \n",
    "\n",
    "        # isolate data in just item group for both dataframes...\n",
    "        l_group = l_df[l_df[p_groupby]==p_item] # geojson l_df data\n",
    "        r_group = r_df[r_df[p_groupby]==p_item] # r_df grouped by county\n",
    "\n",
    "        # for row in L_DATA data... (within this P_GROUP (ie COUNTY DATA)):\n",
    "        for l_idx in l_group.index: # iterate items objects in this group\n",
    "            l_row = l_group.loc[l_idx] \n",
    "\n",
    "            # to store results and break a search when a match is found\n",
    "            match_hist[l_idx] = 0\n",
    "\n",
    "            # search for PERFECT match across ALL COMPARISON COLUMNS:\n",
    "            # this assumes there are no duplicates because it breaks when a match is found\n",
    "\n",
    "        # PERFECT match search\n",
    "            # iterating SHAPE precinct name columns...\n",
    "            for l_compare in l_row.index: \n",
    "                if match_hist[l_idx]: break # break if perfect match found\n",
    "\n",
    "                if any(cu in l_compare for cu in customs):\n",
    "                    continue # ignore found data column\n",
    "\n",
    "                l_val = l_row[l_compare] # SHAPE VALUE NAME\n",
    "                if str(l_val).lower() in ignore_vals: continue\n",
    "\n",
    "                # for row in R_DATA data...\n",
    "                for r_idx in r_group.index: \n",
    "                    if match_hist[l_idx]: break # if search done, break this inner\n",
    "                    r_row = r_group.loc[r_idx]\n",
    "\n",
    "                    # ...compare against both possible precinct name columns\n",
    "                    for r_compare in r_row.index:\n",
    "                        if any(cu in r_compare for cu in customs):\n",
    "                            continue # ignore found data column   \n",
    "                        r_val = r_row[r_compare]\n",
    "                        if str(r_val).lower() in ignore_vals: continue\n",
    "                        if l_val == r_val: # ***CHECK THE L==R VALUE EQUATION***\n",
    "                            meth = f'{l_compare} == {r_compare}'\n",
    "                            \n",
    "                            match_hist[l_idx] = {\n",
    "                                'match_idx': r_idx, 'method': meth }\n",
    "                            \n",
    "                            l_df.loc[l_idx, 'r_idx'] = r_idx\n",
    "                            l_df.loc[l_idx, 'r_mthd'] = meth\n",
    "                            \n",
    "                            break \n",
    "\n",
    "            # end l_row search if perfect match found\n",
    "            if match_hist[l_idx]: continue\n",
    "\n",
    "\n",
    "        # FUZZY match search (if needed)\n",
    "            # minimum score to beat from function input\n",
    "            best_score = min_score-.01\n",
    "            if p_item=='fulton':\n",
    "                best_score = .94\n",
    "        \n",
    "            for l_compare in l_row.index:  # iterating shape columns\n",
    "                if any(cu in l_compare for cu in customs):\n",
    "                    continue  # ignore found data column\n",
    "\n",
    "                # no internal break, search ALL possible scores\n",
    "                #if '_found' in l_compare: continue # ignore found data column\n",
    "\n",
    "                l_val = l_row[l_compare] # SHAPE VALUE NAME\n",
    "                if str(l_val).lower() in ignore_vals: continue\n",
    "\n",
    "                for r_idx in r_group.index: # iterate R_DF rows\n",
    "                    \n",
    "                    if r_idx in l_df['r_idx'].values: continue # already assigned!\n",
    "                        \n",
    "                    # DONT break\n",
    "                    r_row = r_group.loc[r_idx]\n",
    "                    # search ALL possible FUZZY MATCHES...\n",
    "                    for r_compare in r_row.index[1:]:\n",
    "                        \n",
    "                        if any(cu in r_compare for cu in customs):\n",
    "                            continue # ignore results column(s)\n",
    "                            \n",
    "                        if best_score==1: break  # not going to find anything better!\n",
    "\n",
    "                        r_val = r_row[r_compare] # PART VALUE NAME\n",
    "                        if str(r_val).lower() in ignore_vals: continue\n",
    "                        \n",
    "                        # jaro score\n",
    "                        f_score = jaro.jaro_metric(str(l_val), str(r_val))\n",
    "\n",
    "                        if f_score > best_score: # new best score\n",
    "                            best_score = f_score\n",
    "                            f_str = str(round(f_score, 2)).split('.')[1]\n",
    "                            meth = f\"(F.{f_str}) {l_compare} = {r_compare}\"\n",
    "                            match_hist[l_idx] = {\n",
    "                                'match_idx': r_idx,\n",
    "                                'method': meth }\n",
    "                                                    \n",
    "                            l_df.loc[l_idx, 'r_idx'] = r_idx\n",
    "                            l_df.loc[l_idx, 'r_mthd'] = meth\n",
    "                            l_df.loc[l_idx, 'r_fuzz'] = r_val\n",
    "                            # dont break, search all. \n",
    "                            \n",
    "    l_df.replace('nan', pd.np.nan, inplace=True)\n",
    "    r_df.replace('nan', pd.np.nan, inplace=True)\n",
    "\n",
    "    \n",
    "    return l_df, r_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to join all parsed counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(l_df, r_df, dname):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    counties = sorted(list(l_df.county.unique()))\n",
    "    for county in counties:\n",
    "        lc_data = show_county_data(county, l_df)\n",
    "        rc_data = show_county_data(county, r_df)\n",
    "\n",
    "        #lc_data.drop('L_county', axis=1, inplace=True)\n",
    "\n",
    "        c_data = pd.merge(rc_data, lc_data, left_index=True, right_on='r_idx', how='outer')\n",
    "\n",
    "        data = pd.concat([data, c_data])\n",
    "\n",
    "    data['county'] = data['county_x'].fillna(data['county_y']).copy()\n",
    "    if 'prec_id' in data.columns: sorter = 'prec_id'\n",
    "    else: sorter = 'precinct'\n",
    "        \n",
    "    data = data.sort_values(sorter).sort_values('county')\n",
    "    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.drop(columns=['county_x', 'county_y'], inplace=True)\n",
    "    \n",
    "    # get found pct of total possible finds\n",
    "    no_abs = data[(data.prec_desc!='88888') & (data.prec_desc!='99999')]\n",
    "    if 'precinct' in data.columns:\n",
    "        no_abs = data[(data.prec_desc!='88888') & (data.prec_desc!='99999') & (data.precinct.notna())]\n",
    "    not_found = no_abs.r_idx.isna().sum()\n",
    "    fd_pct = round( 100*( (len(no_abs)-not_found)/len(no_abs) ) , 3)\n",
    "    print(f\"{len(no_abs)-not_found}/{len(no_abs)} precincts matched ({fd_pct}%)\")\n",
    "    \n",
    "    data.rename(columns={'r_idx':dname+'_idx'}, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse & merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find matches in PART data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff7d39416bc49e599b05be52ca2a4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2676/2703 precincts matched (99.001%)\n"
     ]
    }
   ],
   "source": [
    "# takes just under 2 minutes with i5\n",
    "l_df, r_df = find_matches(shapes, part, .7)\n",
    "data = join_data(l_df, r_df, 'PART')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find matches in RES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a83828738d482a9354460ee072fd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2672/2672 precincts matched (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# takes around 45 seconds\n",
    "data.drop(['r_mthd', 'r_fuzz'], axis=1, inplace=True)\n",
    "l_df, r_df = find_matches(data, res, .65)\n",
    "\n",
    "# merge the merged\n",
    "merged = join_data(l_df, r_df, 'RES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2672/2672 precincts matched (100.0%)\n"
     ]
    }
   ],
   "source": [
    "merged = join_data(l_df, r_df, 'RES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define name types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cols = list(merged.columns[:7])\n",
    "name_cols.remove('SHAPE_idx')\n",
    "\n",
    "# detect roman numerals to make uppercase\n",
    "thousand = 'M{0,3}'\n",
    "hundred = '(C[MD]|D?C{0,3})'\n",
    "ten = '(X[CL]|L?X{0,3})'\n",
    "digit = '(I[VX]|V?I{0,3})'\n",
    "regex_pattern = r\"%s%s%s%s$\" % (thousand, hundred, ten, digit)\t\n",
    "\n",
    "import re\n",
    "roman_check = lambda x: bool(re.match(regex_pattern, x.upper()))\n",
    "\n",
    "def prettify(string):\n",
    "    x = string.split(' ')\n",
    "    out = []\n",
    "    for y in x:\n",
    "        z = y.split('-')\n",
    "        out.append('-'.join([a.upper() if roman_check(a) or a.lower()=='us)'\n",
    "                             else a.capitalize() for a in z]))\n",
    "    return ' '.join(out)\n",
    "\n",
    "def clean_long(string, county):\n",
    "\n",
    "    for rem in re.findall('[\\d][a-z]', string):\n",
    "        string = string.replace(rem, '')\n",
    "\n",
    "    for rem in re.findall('[\\d]-', string):\n",
    "        string = string.replace(rem, '')\n",
    "\n",
    "    if county in ['chatham', 'columbia']:\n",
    "        return re.sub('[\\d]', '', string).strip()\n",
    "        \n",
    "    else:\n",
    "        return string.strip()\n",
    "\n",
    "for m_idx in merged.index: # iterate all rows\n",
    "    \n",
    "    m_row = merged.loc[m_idx]\n",
    "    \n",
    "    name_vals = [str(m_row[i]).strip() for i in name_cols]\n",
    "    \n",
    "    if '88888' in name_vals:\n",
    "        longest = 'Absentee (domestic)'\n",
    "        shortest = '88888'\n",
    "    elif '99999' in name_vals:\n",
    "        longest = 'Absentee (outside US)'\n",
    "        shortest = '99999'\n",
    "    \n",
    "    else:\n",
    "        longest = ' '*4\n",
    "        shortest = ' '*8\n",
    "        for i, name in enumerate(name_vals):\n",
    "            if name=='nan': continue\n",
    "            if len(name) > len(longest):\n",
    "                longest = clean_long(name, m_row['county']) # remove extra numbers from longest\n",
    "            if len(name) < len(shortest):\n",
    "                shortest = name \n",
    "        shortest = shortest.strip()\n",
    "        longest = longest.strip()\n",
    "        \n",
    "    # remove shortest from longest if overlap\n",
    "    if len(longest) < len(shortest) and m_row['county']!='fayette':\n",
    "        longest = shortest\n",
    "        \n",
    "    elif shortest != longest: # if unique val for shortest...\n",
    "        \n",
    "        if len(shortest)<5 and shortest in longest:\n",
    "            longest.replace(shortest, '')\n",
    "        \n",
    "        if shortest not in longest:\n",
    "            # assign shortest \n",
    "            merged.loc[m_idx, 'precinct_id'] = shortest.upper()    \n",
    "    \n",
    "    \n",
    "        # check for alt names \n",
    "        alt = ''\n",
    "        for i, name in enumerate(name_vals):\n",
    "            if name=='nan': continue\n",
    "            if len(shortest) < len(name) < len(longest):\n",
    "                alt = clean_long(name, m_row['county'])\n",
    "        \n",
    "        if alt and alt not in longest:\n",
    "            merged.loc[m_idx, 'precinct_alt'] = prettify(alt)\n",
    "            \n",
    "    # assign longest\n",
    "    merged.loc[m_idx, 'precinct_name'] = prettify(longest)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop bad guesses\n",
    "#### (when there is a better guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_finds = list(merged.RES_idx)\n",
    "edited = []\n",
    "\n",
    "for find in all_finds:\n",
    "    \n",
    "    these_finds = merged[merged.RES_idx==find]\n",
    "    if len(these_finds)>1:\n",
    "        \n",
    "        best_idx = list(these_finds.sort_values(\n",
    "            'r_mthd', ascending=False).index)[0] # most confident find\n",
    "        \n",
    "        for f_idx in these_finds.index:\n",
    "            if f_idx != best_idx:\n",
    "                merged.loc[f_idx, 'RES_idx'] = pd.np.nan\n",
    "                edited.append(f_idx)\n",
    "                \n",
    "# isolate important columns\n",
    "merged = merged[['county', 'precinct_name', 'precinct_id', 'precinct_alt'] + [c for c in merged.columns if 'idx' in c]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final sweep over long names vs codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ridx in merged[merged.precinct_id.isna()==True].index:\n",
    "    \n",
    "    row = merged.loc[ridx]\n",
    "    if type(merged.loc[ridx, 'precinct_name']) == float:\n",
    "        merged.loc[ridx, 'precinct_name'] = int(merged.loc[ridx, 'precinct_name'])\n",
    "               \n",
    "    p_long = row['precinct_name']\n",
    "    pl_split = p_long.split()\n",
    "    out = ''\n",
    "    if len(pl_split)>1: # multiple broken words\n",
    "        out = ''\n",
    "        for pl in pl_split:\n",
    "            out += pl[0].upper()\n",
    "\n",
    "            has_num = len([r for r in re.findall('[\\d]*', pl) if r!= ''])\n",
    "            if has_num:\n",
    "                out = pl\n",
    "                break\n",
    "                \n",
    "    else: # no space chars\n",
    "        nums = [r for r in re.findall('[\\d]*', p_long) if r!= '']\n",
    "        has_num = len(nums)\n",
    "        \n",
    "        if has_num:\n",
    "            if len(p_long)<=8:\n",
    "                out = p_long\n",
    "            else:\n",
    "                out = nums[0]\n",
    "        else:\n",
    "            continue\n",
    "                \n",
    "    merged.loc[ridx, 'precinct_id'] = out\n",
    "    \n",
    "    if out == merged.loc[ridx, 'precinct_name']:\n",
    "        merged.loc[ridx, 'precinct_name'] = pd.np.nan\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intify(x):\n",
    "    try: return str(int(float(x)))\n",
    "    except: return x\n",
    "    \n",
    "merged.precinct_name = merged.precinct_name.apply(intify)\n",
    "merged.precinct_id = merged.precinct_id.apply(intify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.county = merged.county.apply(prettify)\n",
    "\n",
    "merged['precinct_id'] = merged['precinct_id'].fillna(merged['precinct_name'])\n",
    "merged = merged[[\n",
    "            'county',\n",
    "            'precinct_id',\n",
    "            'precinct_name',\n",
    "            'SHAPE_idx',\n",
    "            'PART_idx',\n",
    "            'RES_idx']]\n",
    "merged.to_csv('unified_precinct_index.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
